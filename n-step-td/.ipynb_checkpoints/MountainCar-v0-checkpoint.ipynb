{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for continious states we use deep qn\n",
    "we can discretize state spaces for now\n",
    "use np.linspace to discretize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def choose_action(self,state):\n",
    "        action = 0\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepSarsa(Agent):\n",
    "    def __init__(self,n,epsilon,alpha,gamma,Q,action_space):\n",
    "        self.n = n\n",
    "        self.epsilon = epsilon \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = Q\n",
    "        self.action_space = action_space\n",
    "    def update(self,g_truncated,next_state,next_action):\n",
    "        prediction = self.Q[next_state + (next_action,)]\n",
    "        target = g_truncated\n",
    "        error = target - prediction\n",
    "        self.Q[next_state+(next_action,)] += self.alpha * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import sys\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "env.observation_space\n",
    "\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "MIN_EXPLORE_RATE = .01\n",
    "MIN_LEARNING_RATE = .2\n",
    "NUM_BUCKETS = (18,14)\n",
    "NUM_EPISODES = 100\n",
    "\n",
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(1, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(MIN_LEARNING_RATE, min(0.5, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "q = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))\n",
    "\n",
    "\n",
    "def get_bucket(state):\n",
    "    [pos,vel] = state\n",
    "    pos_state_idx = int(round(pos+1.2,1)*10)\n",
    "    vel_state_idx = int(round(vel+.07,2)*100)\n",
    "    return tuple([pos_state_idx,vel_state_idx])\n",
    "\n",
    "nStepSarsaAgent = NStepSarsa(1,1,1,1,)\n",
    "for agent in [nStepSarsaAgent]:\n",
    "    for n in [1]:\n",
    "        for alpha in [1]:\n",
    "            agent.n = n\n",
    "            agent.alpha = alpha\n",
    "            agent.gamma = .999\n",
    "            for episode in range(NUM_EPISODES):\n",
    "                obv = env.reet()\n",
    "                state = get_bucket(obv)\n",
    "                action = agent.choose_action(state) \n",
    "                T = sys.maxint\n",
    "                t = 0\n",
    "            \n",
    "                for t in range(T):\n",
    "                    if t < T:\n",
    "                        obv,reward,done,_ = env.step(action)\n",
    "                        if done:\n",
    "                            T = t+1\n",
    "                        else:\n",
    "                            next_state = get_bucket(next_state)\n",
    "                            agent = agent.choose_action(next_state)\n",
    "                    \n",
    "                    theta = t - agent.n + 1\n",
    "                    if theta >= 0:\n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
