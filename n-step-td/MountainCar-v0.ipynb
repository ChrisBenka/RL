{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def choose_action(self,state):\n",
    "        action = 0\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepSarsa(Agent):\n",
    "    def __init__(self,n,epsilon,alpha,gamma,Q,action_space):\n",
    "        self.n = n\n",
    "        self.epsilon = epsilon \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = Q\n",
    "        self.action_space = action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "env.observation_space\n",
    "\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "MIN_EXPLORE_RATE = .01\n",
    "MIN_LEARNING_RATE = .2\n",
    "NUM_BUCKETS = (18,14)\n",
    "NUM_EPISODES = 500\n",
    "DEBUG = False\n",
    "\n",
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(1, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(MIN_LEARNING_RATE, min(0.5, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "q = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))\n",
    "\n",
    "\n",
    "def get_bucket(state):\n",
    "    [pos,vel] = state\n",
    "    pos_state_idx = int(round(pos+1.2,1)*10)\n",
    "    vel_state_idx = int(round(vel+.07,2)*100)\n",
    "    return tuple([pos_state_idx,vel_state_idx])\n",
    "\n",
    "nStepSarsaAgent = NStepSarsa(1,1,1,1,q,env.action_space)\n",
    "\n",
    "REWARDS = {}\n",
    "\n",
    "\n",
    "for n in [2,4,6,8,16,32,64,128,256]:\n",
    "    if DEBUG:\n",
    "        print(\"starting n:\",n)\n",
    "    for alpha in np.linspace(0,1,6):\n",
    "        nStepSarsaAgent.n = n\n",
    "\n",
    "        for episode in range(NUM_EPISODES):\n",
    "\n",
    "            nStepSarsaAgent.alpha = get_learning_rate(episode)\n",
    "            nStepSarsaAgent.epsilon = get_explore_rate(episode)\n",
    "\n",
    "            obv = env.reset()\n",
    "            t = 0\n",
    "            T = np.inf\n",
    "            state = get_bucket(obv)\n",
    "            action = nStepSarsaAgent.choose_action(state)\n",
    "\n",
    "            actions = [action]\n",
    "            states = [state]\n",
    "            rewards = [0]\n",
    "            accum_reward = 0\n",
    "            while True:\n",
    "                if t < T:\n",
    "                    if DEBUG:\n",
    "                        env.render()\n",
    "                    obv,reward,done,_ = env.step(action)\n",
    "                    state = get_bucket(obv)\n",
    "                    accum_reward += reward\n",
    "                    states.append(state)\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    if done:\n",
    "                        T = t +1\n",
    "                    else:\n",
    "                        action = nStepSarsaAgent.choose_action(state)\n",
    "                        actions.append(action)\n",
    "\n",
    "                tau = t - nStepSarsaAgent.n +1\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    for i in range(tau + 1, min(tau + nStepSarsaAgent.n + 1, T + 1)):\n",
    "                        G += np.power(nStepSarsaAgent.gamma, i - tau - 1) * rewards[i]\n",
    "                    if tau + nStepSarsaAgent.n < T:\n",
    "                        state_action = (states[tau + nStepSarsaAgent.n], actions[tau + nStepSarsaAgent.n])\n",
    "                        G += np.power(nStepSarsaAgent.gamma, nStepSarsaAgent.n) * nStepSarsaAgent.Q[state_action[0]][state_action[1]]\n",
    "                    state_action = (states[tau], actions[tau])\n",
    "                    nStepSarsaAgent.Q[state_action[0]][state_action[1]] += nStepSarsaAgent.alpha * (G - nStepSarsaAgent.Q[state_action[0]][state_action[1]])\n",
    "\n",
    "                if tau == T - 1:\n",
    "                    if n not in REWARDS:\n",
    "                        REWARDS[n] = [accum_reward]\n",
    "                    else:\n",
    "                        REWARDS[n].append(accum_reward)\n",
    "                    break\n",
    "                t += 1\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
