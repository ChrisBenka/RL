{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "  \n",
    "class Agent: \n",
    "\n",
    "    def choose_action(self, state):  \n",
    "        action = 0\n",
    "        if np.random.uniform(0, 1) < self.epsilon:  \n",
    "            action = self.action_space.sample() \n",
    "        else: \n",
    "            action = np.argmax(self.Q[state, :])  \n",
    "        return action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(Agent):\n",
    "    \n",
    "    def __init__(self,epislon,alpha,gamma,num_state,num_actions,action_space):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma\n",
    "        self.num_state = num_state\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.Q = np.zeros((num_state,num_actions))\n",
    "        self.action_space = action_space\n",
    "        \n",
    "    def update(self,prev_state,prev_action,reward,next_state,next_action):\n",
    "        predict = self.Q[prev_state,prev_action]\n",
    "        target = reward + self.gamma * self.Q[next_state, next_action] \n",
    "        error = target-predict\n",
    "        self.Q[prev_state,next_state] += alpha*error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    def __init__(self,epislon,alpha,gamma,num_state,num_actions,action_space):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma\n",
    "        self.num_state = num_state\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.Q = np.zeros((num_state,num_actions))\n",
    "        self.action_space = action_space\n",
    "        \n",
    "    def update(self,prev_state,prev_action,reward,next_state,next_action):\n",
    "        predict = self.Q[prev_state,prev_action]\n",
    "        target = reward + self.gamma * np.max(self.Q[next_state,:])\n",
    "        error = target-predict\n",
    "        self.Q[prev_state,next_state] += alpha*error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSarsaAgent(Agent): \n",
    "    def __init__(self, epsilon, alpha, gamma, num_state, num_actions, action_space): \n",
    "        \"\"\" \n",
    "        Contructor \n",
    "        Args: \n",
    "            epsilon: The degree of exploration \n",
    "            gamma: The discount factor \n",
    "            num_state: The number of states \n",
    "            num_actions: The number of actions \n",
    "            action_space: To call the random action \n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon \n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        self.num_state = num_state \n",
    "        self.num_actions = num_actions \n",
    "  \n",
    "        self.Q = np.zeros((self.num_state, self.num_actions)) \n",
    "        self.action_space = action_space \n",
    "    def update(self, prev_state, next_state, reward, prev_action, next_action): \n",
    "        \"\"\" \n",
    "        Update the action value function using the Expected SARSA update. \n",
    "        Q(S, A) = Q(S, A) + alpha(reward + (pi * Q(S_, A_) - Q(S, A)) \n",
    "        Args: \n",
    "            prev_state: The previous state \n",
    "            next_state: The next state \n",
    "            reward: The reward for taking the respective action \n",
    "            prev_action: The previous action \n",
    "            next_action: The next action \n",
    "        Returns: \n",
    "            None \n",
    "        \"\"\"\n",
    "        predict = self.Q[prev_state, prev_action] \n",
    "  \n",
    "        expected_q = 0\n",
    "        q_max = np.max(self.Q[next_state, :]) \n",
    "        greedy_actions = 0\n",
    "        for i in range(self.num_actions): \n",
    "            if self.Q[next_state][i] == q_max: \n",
    "                greedy_actions += 1\n",
    "      \n",
    "        non_greedy_action_probability = self.epsilon / self.num_actions \n",
    "        greedy_action_probability = ((1 - self.epsilon) / greedy_actions) + non_greedy_action_probability \n",
    "  \n",
    "        for i in range(self.num_actions): \n",
    "            if self.Q[next_state][i] == q_max: \n",
    "                expected_q += self.Q[next_state][i] * greedy_action_probability \n",
    "            else: \n",
    "                expected_q += self.Q[next_state][i] * non_greedy_action_probability \n",
    "  \n",
    "        target = reward + self.gamma * expected_q \n",
    "        self.Q[prev_state, prev_action] += self.alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "  \n",
    "class ExpectedSarsaAgent(Agent): \n",
    "    def __init__(self, epsilon, alpha, gamma, num_state, num_actions, action_space): \n",
    "        \"\"\" \n",
    "        Contructor \n",
    "        Args: \n",
    "            epsilon: The degree of exploration \n",
    "            gamma: The discount factor \n",
    "            num_state: The number of states \n",
    "            num_actions: The number of actions \n",
    "            action_space: To call the random action \n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon \n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        self.num_state = num_state \n",
    "        self.num_actions = num_actions \n",
    "  \n",
    "        self.Q = np.zeros((self.num_state, self.num_actions)) \n",
    "        self.action_space = action_space \n",
    "    def update(self, prev_state, next_state, reward, prev_action, next_action): \n",
    "        \"\"\" \n",
    "        Update the action value function using the Expected SARSA update. \n",
    "        Q(S, A) = Q(S, A) + alpha(reward + (pi * Q(S_, A_) - Q(S, A)) \n",
    "        Args: \n",
    "            prev_state: The previous state \n",
    "            next_state: The next state \n",
    "            reward: The reward for taking the respective action \n",
    "            prev_action: The previous action \n",
    "            next_action: The next action \n",
    "        Returns: \n",
    "            None \n",
    "        \"\"\"\n",
    "        predict = self.Q[prev_state, prev_action] \n",
    "  \n",
    "        expected_q = 0\n",
    "        q_max = np.max(self.Q[next_state, :]) \n",
    "        greedy_actions = 0\n",
    "        for i in range(self.num_actions): \n",
    "            if self.Q[next_state][i] == q_max: \n",
    "                greedy_actions += 1\n",
    "      \n",
    "        non_greedy_action_probability = self.epsilon / self.num_actions \n",
    "        greedy_action_probability = ((1 - self.epsilon) / greedy_actions) + non_greedy_action_probability \n",
    "  \n",
    "        for i in range(self.num_actions): \n",
    "            if self.Q[next_state][i] == q_max: \n",
    "                expected_q += self.Q[next_state][i] * greedy_action_probability \n",
    "            else: \n",
    "                expected_q += self.Q[next_state][i] * non_greedy_action_probability \n",
    "  \n",
    "        target = reward + self.gamma * expected_q \n",
    "        self.Q[prev_state, prev_action] += self.alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 24 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f050397e309c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# Learning the Q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mstate1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-d8f33064e963>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, prev_state, prev_action, reward, next_state, next_action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_action\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 24 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "  \n",
    "from matplotlib import pyplot as plt \n",
    "  \n",
    "# Using the gym library to create the environment \n",
    "env = gym.make('CliffWalking-v0') \n",
    "  \n",
    "# Defining all the required parameters \n",
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "max_steps = 100\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "\"\"\" \n",
    "    The two parameters below is used to calculate \n",
    "    the reward by each algorithm \n",
    "\"\"\"\n",
    "episodeReward = 0\n",
    "totalReward = { \n",
    "    'SarsaAgent': [], \n",
    "    'QLearningAgent': [], \n",
    "    'ExpectedSarsaAgent': [] \n",
    "} \n",
    "  \n",
    "# Defining all the three agents \n",
    "expectedSarsaAgent = ExpectedSarsaAgent( \n",
    "    epsilon, alpha, gamma, env.observation_space.n,  \n",
    "    env.action_space.n, env.action_space) \n",
    "qLearningAgent = QLearningAgent( \n",
    "    epsilon, alpha, gamma, env.observation_space.n,  \n",
    "    env.action_space.n, env.action_space) \n",
    "sarsaAgent = SarsaAgent( \n",
    "    epsilon, alpha, gamma, env.observation_space.n,  \n",
    "    env.action_space.n, env.action_space) \n",
    "  \n",
    "# Now we run all the episodes and calculate the reward obtained by \n",
    "# each agent at the end of the episode \n",
    "  \n",
    "agents = [expectedSarsaAgent, qLearningAgent, sarsaAgent] \n",
    "  \n",
    "for agent in agents: \n",
    "    for _ in range(total_episodes): \n",
    "        # Initialize the necesary parameters before  \n",
    "        # the start of the episode \n",
    "        t = 0\n",
    "        state1 = env.reset()  \n",
    "        action1 = agent.choose_action(state1)  \n",
    "        episodeReward = 0\n",
    "        while t < max_steps: \n",
    "  \n",
    "            # Getting the next state, reward, and other parameters \n",
    "            state2, reward, done, info = env.step(action1)  \n",
    "      \n",
    "            # Choosing the next action  \n",
    "            action2 = agent.choose_action(state2)  \n",
    "              \n",
    "            # Learning the Q-value  \n",
    "            agent.update(state1, state2, reward, action1, action2)  \n",
    "      \n",
    "            state1 = state2  \n",
    "            action1 = action2  \n",
    "              \n",
    "            # Updating the respective vaLues  \n",
    "            t += 1\n",
    "            episodeReward += reward \n",
    "              \n",
    "            # If at the end of learning process  \n",
    "            if done:  \n",
    "                break\n",
    "        # Append the sum of reward at the end of the episode \n",
    "        totalReward[type(agent).__name__].append(episodeReward) \n",
    "env.close() \n",
    "  \n",
    "# Calculate the mean of sum of returns for each episode \n",
    "meanReturn = { \n",
    "    'SARSA-Agent': np.mean(totalReward['SarsaAgent']), \n",
    "    'Q-Learning-Agent': np.mean(totalReward['QLearningAgent']), \n",
    "    'Expected-SARSA-Agent': np.mean(totalReward['ExpectedSarsaAgent']) \n",
    "} \n",
    "  \n",
    "# Print the results \n",
    "print(f\"SARSA Average Sum of Reward: {meanReturn['SARSA-Agent']}\") \n",
    "print(f\"Q-Learning Average Sum of Return: {meanReturn['Q-Learning-Agent']}\") \n",
    "print(f\"Expected Sarsa Average Sum of Return: {meanReturn['Expected-SARSA-Agent']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
