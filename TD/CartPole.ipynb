{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def choose_action(self,state):\n",
    "        action = 0\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "class DoubleLearningAgent:\n",
    "    def choose_action(self,state):\n",
    "        action = 0\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q1[state]+self.Q2[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxObservationSarsaAgent(Agent):\n",
    "    def __init__(self,epsilon,alpha,gamma,Q,action_space):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.action_space = action_space\n",
    "        self.Q = Q\n",
    "    def update(self,prev_state,prev_action,reward,next_state,next_action):\n",
    "        prediction = self.Q[prev_state+(prev_action,)]\n",
    "        target = reward + self.gamma * self.Q[next_state+(next_action,)]\n",
    "        error = target - prediction\n",
    "        self.Q[prev_state+(prev_action,)] += self.alpha * error\n",
    "        \n",
    "class BoxObservationQLearningAgent(Agent):\n",
    "    def __init__(self,epsilon,alpha,gamma,Q,action_space):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.action_space = action_space\n",
    "        self.Q = Q\n",
    "    def update(self,prev_state,prev_action,reward,next_state,next_action):\n",
    "        prediction = self.Q[prev_state+(prev_action,)]\n",
    "        target = reward + self.gamma * np.amax(self.Q[next_state])\n",
    "        error = target - prediction\n",
    "        self.Q[prev_state+(prev_action,)] += self.alpha * error\n",
    "\n",
    "class BoxObservationDoubleQLearningAgent(DoubleLearningAgent):\n",
    "    def __init__(self,epsilon,alpha,gamma,Q1,Q2,action_space):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.Q1 = Q1\n",
    "        self.Q2 = Q2\n",
    "\n",
    "        self.action_space = action_space\n",
    "    def update(self,prev_state,prev_action,reward,next_state,next_action):\n",
    "        if np.random.uniform(0,1) < .5:\n",
    "            prediction = self.Q1[prev_state+(prev_action,)]\n",
    "            target = reward + self.gamma*self.Q2[next_state+(np.argmax(self.Q1[next_state]),)]\n",
    "            error = target - prediction\n",
    "            self.Q1[prev_state+(prev_action,)] += self.alpha * error\n",
    "        else:\n",
    "            prediction = self.Q2[prev_state+(prev_action,)]\n",
    "            target = reward + self.gamma*self.Q1[next_state+(np.argmax(self.Q2[next_state]),)]\n",
    "            error = target - prediction\n",
    "            self.Q2[prev_state+(prev_action,)] += self.alpha * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "NUM_BUCKETS = (1, 1, 6, 3)  # (x, x', theta, theta')\n",
    "NUM_ACTIONS = env.action_space.n # (left, right)\n",
    "# Bounds for each discrete state\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "# Manually setting bounds (needed for the x_dot and theta_dot)\n",
    "#STATE_BOUNDS[0] = [STATE_BOUNDS[0][0]/2, STATE_BOUNDS[0][1]/2]\n",
    "STATE_BOUNDS[1] = [-0.5, 0.5]\n",
    "#STATE_BOUNDS[2] = [STATE_BOUNDS[2][0]/2, STATE_BOUNDS[2][0]/2]\n",
    "STATE_BOUNDS[3] = [-math.radians(50), math.radians(50)]\n",
    "\n",
    "## Learning related constants\n",
    "MIN_EXPLORE_RATE = 0.01\n",
    "MIN_LEARNING_RATE = 0.2\n",
    "\n",
    "# Defining the simulation related constants\n",
    "NUM_EPISODES = 500\n",
    "MAX_T = 500\n",
    "STREAK_TO_END = 100\n",
    "SOLVED_T = 199\n",
    "\n",
    "NUM_TRIALS = 1\n",
    "DEBUG = False\n",
    "\n",
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(1, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(MIN_LEARNING_RATE, min(0.5, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "\n",
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i] - 1\n",
    "        else:\n",
    "            # Mapping the state bounds to the bucket array\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (NUM_BUCKETS[i]-1)*STATE_BOUNDS[i][0]/bound_width\n",
    "            scaling = (NUM_BUCKETS[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)\n",
    "\n",
    "\n",
    "boxQAgent = BoxObservationQLearningAgent(1,1,1,np.zeros(NUM_BUCKETS + (NUM_ACTIONS,)),env.action_space)\n",
    "boxDoubleQAgent = BoxObservationDoubleQLearningAgent(1,1,1,np.zeros(NUM_BUCKETS + (NUM_ACTIONS,)),np.zeros(NUM_BUCKETS + (NUM_ACTIONS,)),env.action_space)\n",
    "boxSarsa = BoxObservationSarsaAgent(1,1,1,np.zeros(NUM_BUCKETS + (NUM_ACTIONS,)),env.action_space)\n",
    "agents = [boxQAgent,boxDoubleQAgent,boxSarsa]\n",
    "\n",
    "episdoesRequiredToSolve = {\n",
    "    'BoxObservationSarsaAgent': [],\n",
    "    'BoxObservationQLearningAgent': [],\n",
    "    'BoxObservationDoubleQLearningAgent':[]\n",
    "}\n",
    "for agent in agents:\n",
    "    for trial in range(NUM_TRIALS):\n",
    "        agent.alpha = get_learning_rate(0)\n",
    "        agent.epsilon = get_explore_rate(0)\n",
    "        agent.gamma = 0.999  # since the world is unchanging\n",
    "\n",
    "        num_streaks = 0\n",
    "\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            obv = env.reset()\n",
    "\n",
    "            state = state_to_bucket(obv)\n",
    "\n",
    "            for t in range(MAX_T):\n",
    "                if DEBUG:\n",
    "                    env.render()\n",
    "\n",
    "                action = agent.choose_action(state)\n",
    "\n",
    "                obv,reward,done,_ = env.step(action)\n",
    "\n",
    "                next_state = state_to_bucket(obv)\n",
    "\n",
    "                next_action = agent.choose_action(next_state)\n",
    "\n",
    "                agent.update(state,action,reward,next_state,next_action)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    if DEBUG:\n",
    "                        print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "                    if t >= SOLVED_T:\n",
    "                        num_streaks += 1\n",
    "                    else:\n",
    "                        num_streaks = 0\n",
    "                    break\n",
    "            if num_streaks > STREAK_TO_END:\n",
    "                episdoesRequiredToSolve[type(agent).__name__].append(episode)\n",
    "                break\n",
    "            elif episode == NUM_EPISODES:\n",
    "                episdoesRequiredToSolve[type(agent).__name__].append(NUM_EPISODES)\n",
    "            agent.epsilon = get_explore_rate(episode)\n",
    "            agent.alpha = get_learning_rate(episode)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoxObservationQLearningAgent [274]\n",
      "BoxObservationDoubleQLearningAgent [283]\n",
      "BoxObservationSarsaAgent []\n"
     ]
    }
   ],
   "source": [
    "for agent in agents:\n",
    "    print(type(agent).__name__,episdoesRequiredToSolve[type(agent).__name__])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
