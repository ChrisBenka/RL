{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def choose_action(self,state):\n",
    "        action = 0\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "class DoubleLearningAgent:\n",
    "    def choose_action(self,state):\n",
    "        action = 0\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q1[state]+self.Q2[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa(Agent):\n",
    "    def __init__(self,epsilon,alpha,gamma,Q,action_space):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma\n",
    "        self.Q = Q\n",
    "        self.action_space = action_space\n",
    "    def update(self,prev_state,prev_action,reward,next_state,next_action):\n",
    "        prediction = self.Q[prev_state + (prev_action,)]\n",
    "        target = reward + self.gamma * self.Q[next_state+(next_action,)]\n",
    "        error = target - prediction\n",
    "        self.Q[prev_state+(prev_action,)] += self.alpha * error\n",
    "class QLearner(Agent):\n",
    "    def __init__(self,epsilon,alpha,gamma,Q,action_space):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma\n",
    "        self.Q = Q\n",
    "        self.action_space = action_space\n",
    "    def update(self,prev_state,prev_action,reward,next_state,next_action):\n",
    "        prediction = self.Q[prev_state+(prev_action,)]\n",
    "        target = reward + self.gamma * np.amax(self.Q[next_state])\n",
    "        error = target - prediction\n",
    "        self.Q[prev_state+(prev_action,)] += self.alpha * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "env.observation_space\n",
    "\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "MIN_EXPLORE_RATE = .01\n",
    "MIN_LEARNING_RATE = .2\n",
    "NUM_BUCKETS = (180,14)\n",
    "NUM_EPISODES = 1000\n",
    "MAX_STEPS = 100\n",
    "\n",
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(1, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(MIN_LEARNING_RATE, min(0.5, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "q = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))\n",
    "\n",
    "\n",
    "def get_bucket(state):\n",
    "    [pos,vel] = state\n",
    "    pos_state_idx = int(round(pos+1.2,1)*100)\n",
    "    vel_state_idx = int(round(vel+.07,2)*100)\n",
    "    return tuple([pos_state_idx,vel_state_idx])\n",
    "\n",
    "\n",
    "sarsaAgent = QLearner(1,1,1,q,env.action_space)\n",
    "\n",
    "agents = [sarsaAgent]\n",
    "\n",
    "for agent in agents:\n",
    "    agent.alpha = get_learning_rate(0)\n",
    "    agent.epsilon = get_explore_rate(0)\n",
    "    agent.gamma = 0.999\n",
    "    \n",
    "    for episode in range(NUM_EPISODES):\n",
    "        obv = env.reset()\n",
    "        state = get_bucket(obv)\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.choose_action(state)\n",
    "            obv,reward,done,_ = env.step(action)\n",
    "            next_state = get_bucket(obv)\n",
    "            next_action = agent.choose_action(next_state)\n",
    "            agent.update(state,action,reward,next_state,next_action)\n",
    "            state = next_state\n",
    "        \n",
    "        agent.alpha = get_learning_rate(episode)\n",
    "        agent.epsilon = get_explore_rate(episode)\n",
    "\n",
    "env.close()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
