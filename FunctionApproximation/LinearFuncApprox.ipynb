{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "import itertools\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n",
    "from lib import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFeaturizer():\n",
    "    def __init__(self,env):\n",
    "        observation_exs = np.array([env.observation_space.sample() for _ in range(10000)])\n",
    "        self.scaler = sklearn.preprocessing.StandardScaler()\n",
    "        #Standardize features by removing the mean and scaling to unit variance\n",
    "        self.scaler.fit(observation_exs)\n",
    "        \n",
    "        self.featurizer = sklearn.pipeline.FeatureUnion([\n",
    "            (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "            (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "            (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "            (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "        \n",
    "        self.featurizer.fit(self.scaler.transform(observation_exs))\n",
    "        self.num_weights = 400\n",
    "    def featurize(self,observation):\n",
    "        scaled = self.scaler.transform([observation])\n",
    "        return self.featurizer.transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActionValueEstimator():\n",
    "    def __init__(self,featurizer,nA):\n",
    "        self.featurizer = featurizer\n",
    "        self.nA = nA\n",
    "        self.weights = np.zeros([nA,self.featurizer.num_weights])\n",
    "    def featurize_state(self,state):\n",
    "        return self.featurizer.featurize(state)[0]\n",
    "    def predict(self,state,a=None):\n",
    "        state_features = self.featurize_state(state)\n",
    "        if a is not None:\n",
    "            return state_features.dot(self.weights[a])\n",
    "        else:\n",
    "            return np.array([state_features.dot(a) for a in range(self.nA)])\n",
    "    def update(self, s, a, y):\n",
    "            \"\"\"\n",
    "            Updates the estimator parameters for a given state and action towards\n",
    "            the target y.\n",
    "            \"\"\"\n",
    "            alpha = .01\n",
    "            state_featurized = self.featurize_state(s)\n",
    "            self.weights[a] += alpha * (y-self.predict(s,a)) * state_featurized\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(observation)\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_featurizer = RadialBasisFeaturizer(env)\n",
    "fa = LinearActionValueEstimator(rbf_featurizer,env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env,estimator,num_episodes,discount_factor=1.0,epsilon=.1,epsilon_decay=1.0):\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        policy = make_epsilon_greedy_policy(estimator,epsilon,env.action_space.n)\n",
    "        \n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arrange(len(action_probs)),p=action_probs)\n",
    "            \n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            \n",
    "            episode_rewards[episode] += reward\n",
    "            episode_lengths[episode] = t\n",
    "            \n",
    "            q_values_next = estimator.predict(next_state)            \n",
    "                        \n",
    "            action_probs = policy(next_state)\n",
    "            \n",
    "            next_action = np.random.choice(np.arrange(len(action_probs)),p=action_probs)\n",
    "            \n",
    "            target = reward + discount_factor * q_values_next[next_action]\n",
    "            \n",
    "            estimator.update(state,action,target)\n",
    "            \n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(t, episode + 1, num_episodes, last_reward), end=\"\")\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1059 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-73166cbd78be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msarsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-8a71e1298c25>\u001b[0m in \u001b[0;36msarsa\u001b[0;34m(env, estimator, num_episodes, discount_factor, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-4ee55e01bee6>\u001b[0m in \u001b[0;36mpolicy_fn\u001b[0;34m(observation)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1059 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "stats = sarsa(env,fa,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
