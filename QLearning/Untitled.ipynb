{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath  /Users/chrisbenka/ML/rl/RL/QLearning/LL-QL-v2-weights.h5\n",
      "File  LL-QL-v2-weights.h5  does not exis. Retraining... \n",
      "dataX shape (1, 12)\n",
      "dataY shape (1, 1)\n",
      "Training Game # 0  steps =  59 last reward -100  finished with headscore  -67.43135134360939\n",
      "Training Game # 5  steps =  103 last reward -100  finished with headscore  -46.60588971563469\n",
      "Training Game # 10  steps =  66 last reward -100  finished with headscore  -20.499267920654066\n",
      "WARNING:tensorflow:From /Users/chrisbenka/opt/anaconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LunarLander-v2 solution by Michel Aka\n",
    "https://github.com/FitMachineLearning/FitML/\n",
    "https://www.youtube.com/channel/UCi7_WxajoowBl4_9P0DhzzA/featured\n",
    "Using Modified Q Learning, Bellman, Reinforcement Learning, RL memory\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import keras\n",
    "import gym\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "num_env_variables = 8\n",
    "num_env_actions = 4\n",
    "num_initial_observation = 15\n",
    "learning_rate = 0.003\n",
    "weigths_filename = \"LL-QL-v2-weights.h5\"\n",
    "\n",
    "b_discount = 0.99\n",
    "max_memory_len = 60000\n",
    "starting_explore_prob = 0.05\n",
    "training_epochs = 3\n",
    "load_previous_weights = True\n",
    "observe_and_train = True\n",
    "save_weights = True\n",
    "num_games_to_play = 1000\n",
    "\n",
    "\n",
    "#One hot encoding array\n",
    "possible_actions = np.arange(0,num_env_actions)\n",
    "actions_1_hot = np.zeros((num_env_actions,num_env_actions))\n",
    "actions_1_hot[np.arange(num_env_actions),possible_actions] = 1\n",
    "\n",
    "#Create testing enviroment\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "#initialize training matrix with random states and actions\n",
    "dataX = np.random.random(( 5,num_env_variables+num_env_actions ))\n",
    "#Only one output for the total score\n",
    "dataY = np.random.random((5,1))\n",
    "\n",
    "\n",
    "\n",
    "#nitialize the Neural Network with random weights\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Dense(num_env_variables+num_env_actions, activation='tanh', input_dim=dataX.shape[1]))\n",
    "model.add(Dense(512, activation='relu', input_dim=dataX.shape[1]))\n",
    "model.add(Dense(256, activation='relu' ))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(dataY.shape[1]))\n",
    "\n",
    "opt = optimizers.adam(lr=learning_rate)\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#load previous model weights if they exist\n",
    "if load_previous_weights:\n",
    "    dir_path = os.path.realpath(\".\")\n",
    "    fn = dir_path + \"/\"+weigths_filename\n",
    "    print(\"filepath \", fn)\n",
    "    if  os.path.isfile(fn):\n",
    "        print(\"loading weights\")\n",
    "        model.load_weights(weigths_filename)\n",
    "    else:\n",
    "        print(\"File \",weigths_filename,\" does not exis. Retraining... \")\n",
    "\n",
    "#Initialize training data array\n",
    "total_steps = 0\n",
    "dataX = np.zeros(shape=(1,num_env_variables+num_env_actions))\n",
    "dataY = np.zeros(shape=(1,1))\n",
    "\n",
    "#Initialize Memory Array data array\n",
    "memoryX = np.zeros(shape=(1,num_env_variables+num_env_actions))\n",
    "memoryY = np.zeros(shape=(1,1))\n",
    "\n",
    "\n",
    "print(\"dataX shape\", dataX.shape)\n",
    "print(\"dataY shape\", dataY.shape)\n",
    "\n",
    "\n",
    "#This function predicts the reward that will result from taking an \"action\" at a state \"qstate\"\n",
    "def predictTotalRewards(qstate, action):\n",
    "    qs_a = np.concatenate((qstate,actions_1_hot[action]), axis=0)\n",
    "    predX = np.zeros(shape=(1,num_env_variables+num_env_actions))\n",
    "    predX[0] = qs_a\n",
    "\n",
    "    #print(\"trying to predict reward at qs_a\", predX[0])\n",
    "    pred = model.predict(predX[0].reshape(1,predX.shape[1]))\n",
    "    remembered_total_reward = pred[0][0]\n",
    "    return remembered_total_reward\n",
    "\n",
    "\n",
    "\n",
    "if observe_and_train:\n",
    "\n",
    "    #Play the game a determine number of times\n",
    "    for game in range(num_games_to_play):\n",
    "        gameX = np.zeros(shape=(1,num_env_variables+num_env_actions))\n",
    "        gameY = np.zeros(shape=(1,1))\n",
    "        #Get the initial Q state\n",
    "        qs = env.reset()\n",
    "        for step in range (40000):\n",
    "\n",
    "            #Learn from observation and not playing\n",
    "            if game < num_initial_observation:\n",
    "                #take a radmon action\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                #Now playing and also learning from experience during play\n",
    "\n",
    "                #Calculate probability to take deterministic action vs random action (epsilon)\n",
    "                prob = np.random.rand(1)\n",
    "                explore_prob = starting_explore_prob-(starting_explore_prob/num_games_to_play)*game\n",
    "\n",
    "                #Chose between prediction and chance\n",
    "                if prob < explore_prob:\n",
    "                    #take a random action\n",
    "                    a=env.action_space.sample()\n",
    "                    #print(\"taking random action\",a, \"at total_steps\" , total_steps)\n",
    "                    #print(\"prob \", prob, \"explore_prob\", explore_prob)\n",
    "\n",
    "                else:\n",
    "                    ##chose an action by estimating the function-estimator remembered consequences of all possible actions\n",
    "                    ## Bellman states that the best policy (i.e. action) is the one that maximizez expected rewards for future states\n",
    "                    ## to caculate rewards we compute the reward a this state t + the discounted (b_discount) reward at all possible state t+1\n",
    "                    ## all states t+1 are estimated by our function estimator (our Neural Network)\n",
    "\n",
    "\n",
    "                    utility_possible_actions = np.zeros(shape=(num_env_actions))\n",
    "\n",
    "                    utility_possible_actions[0] = predictTotalRewards(qs,0)\n",
    "                    utility_possible_actions[1] = predictTotalRewards(qs,1)\n",
    "                    utility_possible_actions[2] = predictTotalRewards(qs,2)\n",
    "                    utility_possible_actions[3] = predictTotalRewards(qs,3)\n",
    "\n",
    "\n",
    "                    #chose argmax action of estimated anticipated rewards\n",
    "                    #print(\"utility_possible_actions \",utility_possible_actions)\n",
    "                    #print(\"argmax of utitity\", np.argmax(utility_possible_actions))\n",
    "                    a = np.argmax(utility_possible_actions)\n",
    "\n",
    "\n",
    "\n",
    "            env.render()\n",
    "            qs_a = np.concatenate((qs,actions_1_hot[a]), axis=0)\n",
    "\n",
    "            #print(\"action\",a,\" qs_a\",qs_a)\n",
    "            #Perform the optimal action and get the target state and reward\n",
    "            s,r,done,info = env.step(a)\n",
    "\n",
    "\n",
    "            #record information for training and memory\n",
    "            if step ==0:\n",
    "                gameX[0] = qs_a\n",
    "                gameY[0] = np.array([r])\n",
    "                memoryX[0] = qs_a\n",
    "                memoryY[0] = np.array([r])\n",
    "\n",
    "            gameX = np.vstack((gameX,qs_a))\n",
    "            gameY = np.vstack((gameY,np.array([r])))\n",
    "\n",
    "\n",
    "            if done :\n",
    "                #GAME ENDED\n",
    "                #Calculate Q values from end to start of game (From last step to first)\n",
    "                for i in range(0,gameY.shape[0]):\n",
    "                    #print(\"Updating total_reward at game epoch \",(gameY.shape[0]-1) - i)\n",
    "                    if i==0:\n",
    "                        #print(\"reward at the last step \",gameY[(gameY.shape[0]-1)-i][0])\n",
    "                        gameY[(gameY.shape[0]-1)-i][0] = gameY[(gameY.shape[0]-1)-i][0]\n",
    "                    else:\n",
    "                        #print(\"local error before Bellman\", gameY[(gameY.shape[0]-1)-i][0],\"Next error \", gameY[(gameY.shape[0]-1)-i+1][0])\n",
    "                        gameY[(gameY.shape[0]-1)-i][0] = gameY[(gameY.shape[0]-1)-i][0]+b_discount*gameY[(gameY.shape[0]-1)-i+1][0]\n",
    "                        #print(\"reward at step\",i,\"away from the end is\",gameY[(gameY.shape[0]-1)-i][0])\n",
    "                    if i==gameY.shape[0]-1 and game%5==0:\n",
    "                        print(\"Training Game #\",game, \" steps = \", step ,\"last reward\", r,\" finished with headscore \", gameY[(gameY.shape[0]-1)-i][0])\n",
    "\n",
    "                if memoryX.shape[0] ==1:\n",
    "                    memoryX = gameX\n",
    "                    memoryY = gameY\n",
    "                else:\n",
    "                    #Add experience to memory\n",
    "                    memoryX = np.concatenate((memoryX,gameX),axis=0)\n",
    "                    memoryY = np.concatenate((memoryY,gameY),axis=0)\n",
    "\n",
    "                #if memory is full remove first element\n",
    "                if np.alen(memoryX) >= max_memory_len:\n",
    "                    #print(\"memory full. mem len \", np.alen(memoryX))\n",
    "                    for l in range(np.alen(gameX)):\n",
    "                        memoryX = np.delete(memoryX, 0, axis=0)\n",
    "                        memoryY = np.delete(memoryY, 0, axis=0)\n",
    "\n",
    "            #Update the states\n",
    "            qs=s\n",
    "\n",
    "            #Retrain every X game after num_initial_observation\n",
    "            if done and game >= num_initial_observation:\n",
    "                if game%10 == 0:\n",
    "                    print(\"Training  game# \", game,\"momory size\", memoryX.shape[0])\n",
    "                    model.fit(memoryX,memoryY, batch_size=256,nb_epoch=training_epochs,verbose=0)\n",
    "\n",
    "            if done:\n",
    "                if r >= 0 and r <99:\n",
    "                    print(\"Game \",game,\" ended with positive reward \")\n",
    "                if r > 50:\n",
    "                    print(\"Game \", game,\" WON *** \" )\n",
    "                #Game ended - Break\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if save_weights:\n",
    "    #Save model\n",
    "    print(\"Saving weights\")\n",
    "    model.save_weights(weigths_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml env",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
